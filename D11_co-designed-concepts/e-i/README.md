# E-I
## Evaluation interface

E-I is technology (hardware + software) created to increase the ability to reuse materials. Its users show objects to the machine. E-I identifies the objects and displays information about their repairability, spare parts, raw materials, second-hand market, possibilities of upcycling, adaptations and transformations, user stories, etc.

# 1. Background

E-I is a new iteration of an earlier concept called [Point and Reuse](https://web.archive.org/web/*/https://is.efeefe.me/concepts/point-reuse) (an app for mobile devices), extended and transformed into different form factors: a workbench version and a kiosk one.

# 2. Updated concept

During the second year of research, the requirement to work and prototype with open hardware informed the transformation of Point and Reuse into a physical machine to help assess the reuse potential of goods and objects. Intermediary versions called the machine e-valudata, associating it explicitly with the tentative name - valudata - for the universal registry of things. Eventually those names were dropped and after [some brainstorming](../naming.md), E-I was the chosen identity for this concept.

The goal with E-I at this point is not to develop a functional product, but rather to discuss how can digital information technologies help society reuse a larger proportion of materials that are currently discarded, and what would the implications of such technologies be in terms of use, privacy, health and safety, policy and economy.

# 3. Prototyping

The expected behaviour of the workbench version of E-I is designed around the steps below:

 1. The user presents a specific object to E-I in one of the following ways:
placing the object on E-I
  - typing a search query into a keyboard in its touchscreen, or
  - asking E-I about the object.
 2. E-I compares images and other information collected from the product/object (weight, colors, barcode, QR code) against an open database of things
 3. When information about the object is found, E-I retrieves it and delivers to the user via screen or voice.
 4. When information about the object is not found, E-I allows the user to input it via keyboard or voice.


A prototype of E-I was developed during the second year. Documentation can be found [here](../D12_documentation-of-prototypes/e-i).

# 4. References

## 4.1. Monk - Ai-based inspection system

> Creating Trust whenever an item changes hands.

 - [Website](https://web.archive.org/web/*/https://monkvision.ai/#how-it-works)
 - [Repository](https://web.archive.org/web/*/https://github.com/Tessellate-Imaging/monk_v1).

## 4.2. [Teachable Machine]

> Train a computer to recognize your own images, sounds, & poses.

 - [Website](https://web.archive.org/web/*/https://teachablemachine.withgoogle.com/)

## 4.3. Google lens

> Search what you see

 - [Website](https://web.archive.org/web/*/https://lens.google.com)

## 4.4. ODK - Object Detection Kit

> Convert garbage into resources The garbage on our streets can have a lot of value when it is repurposed. ODK aims to take a step forward into circularity, making it easier to give garbage a new life.

 - [Website](https://web.archive.org/web/*/https://odk.ai)

## 4.5. Pi Trash Classifier

> Build a custom trash classifier using the Raspberry Pi and Lobe, a beginner-friendly machine learning program (without coding).

 - [Website](https://web.archive.org/web/*/https://hackster.io/jenfoxbot)

## 4.6. Plant identification apps

- [List of apps](https://web.archive.org/web/20210127032917/https://youhadmeatgardening.com/best-plant-identification-app/):
  - PlantNet
  - iNaturalist
  - PlantSnap
  - PictureThis
  - FlowerChecker
  - Garden Compass
  - Agrobase
  - Plantix
  - Whatâ€™s That Flower
